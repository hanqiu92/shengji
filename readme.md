# 训练打拖拉机程序

包括打明牌和打简化牌组两部分尝试。

## 建模思路简述

### state处理方式

* 明牌：不同agent状态（共13种）下的deck，共享一组处理方式（suit+size作为one-hot输入项，然后基于num/level用卷积），好处是$s$和$a$可以共用一套处理机制，正则性强；缺点是表达能力弱。
* 暗牌：对agent状态进行embedding，然后将embedding结果和suit+size生成的one-hot信息合并，整体进行卷积处理。好处是表达能力强，缺点是容易过拟合。

### direc处理方式

方向direc的作用：direc表征当前agent的预测方向是否与第一个agent（编号为0）的预测方向一致。

* 对于regrets/policy/logit，由于agent本身不变，故方向direc不影响结果；
* 而对于value，因为value是相对于position定义的，因此方向direc影响结果。
  
因此，在Q-based模型中，由于通过value来训练和定义policy，因此需要引入direc在value和policy之间转换；而在CFR-based模型中，是直接训练regrets和policy的，故direc的引入意义不大。

## 现存问题

### 明牌场景

1. 数据采样效率低，而值函数的输入特征较为复杂，因此在有限时间内很难得到充足的样本，导致模型容易过拟合。一种可能的解决方案时复用多次迭代步中产生的数据，并增加检验集以控制模型学习步数。
2. 由于仿真速度较慢，MCTS中只能选取较少的采样次数，导致许多动作选项未得到充分探索，容易受到值函数或者策略函数误差的影响，从而影响最终的决策效果。一种可能的改进方法是在当前的值预测中，将“已发生真实值+预测未来值”形式改为“预测总值”，避免在搜索树中对不同深度的结点引入结构性偏差（例如，预测未来值皆为0时，搜索深度越深，“已发生真实值+预测未来值”会越大/越小）。

### （简化）暗牌场景

从评估结果看，DQN > CFR > FP > random。猜测CFR和FP效果不佳的主要原因在于两点：
1. 相比regret，平均policy的训练数据量较少，训练不充分；
2. policy形式导致action的确定性不够，在拖拉机这种场景会有比较大的表现损失。（可能充分训练后该问题会有改善。）
